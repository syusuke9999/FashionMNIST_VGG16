{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FashionMNIST_VGG16",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syusuke9999/FashionMNIST_VGG16/blob/main/FashionMNIST_VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-s6bVfwPQ40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "2bb0966b-ce94-4029-a23e-d309a7281c6d"
      },
      "source": [
        "import cv2\n",
        "import keras\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.applications import VGG16\n",
        "import numpy as np\n",
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-feff1fc14a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'VGG16' from 'tensorflow.python.keras.applications' (/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/applications/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1hPM12mPT04"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_CXBBsbPXss"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6-R722jPfvE"
      },
      "source": [
        "**パラメーターの設定**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIlHOyCCPZ5B"
      },
      "source": [
        "batch_size = 128\n",
        "num_class = 10\n",
        "epochs = 100\n",
        "image_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVuN4zRDPoaK"
      },
      "source": [
        "#  keras.datasets.fashion_mnistからデータを読み込む。\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhfGKEF8X9n9"
      },
      "source": [
        "#  それぞれの分類をリストに保存する\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxqOZZt9Puyn"
      },
      "source": [
        "####  VGG16のトレーニングモデルは最低限32×32のサイズとRGBカラーチャネルを持ったデータでないと受け付けないため、\n",
        "#### リスト内包表記とopenCVの関数を使用して、画像サイズを拡大し、カラーチャネルをRGBの3つに増やす。\n",
        "####  x_trainからimgを1つずつ取り出し、画像サイズの変換とRGBデータへの変換を同時に行い、numpy.ndarray形式に変換して、x_trainに代入する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvw7-SP9Pr7G"
      },
      "source": [
        "x_train = np.array(\n",
        "    [cv2.cvtColor(cv2.resize(img, (image_size, image_size)),\n",
        "                  cv2.COLOR_GRAY2RGB) for img in x_train])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJBKOAE-P0a1"
      },
      "source": [
        "###  x_testについても同様の操作を行う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB80E9cqPxZD"
      },
      "source": [
        "x_test = np.array(\n",
        "    [cv2.cvtColor(cv2.resize(img, (image_size, image_size)),\n",
        "                  cv2.COLOR_GRAY2RGB) for img in x_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqCueUGqP59w"
      },
      "source": [
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    plt.title(\"Label: \" + str(i))\n",
        "    plt.imshow(x_train[i].reshape(image_size, image_size, 3), cmap=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVIhnSQOP7d8"
      },
      "source": [
        "# 前処理として画像データを0～1の間の数値に正規化する\n",
        "x_train, x_test = x_train / 255.0 , x_test / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU-2nnV3MifA"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOkj1DdZP9Gi"
      },
      "source": [
        "# 最適化手法の指定\n",
        "opt = Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSBPxsvwQB0d"
      },
      "source": [
        "# 事前学習に'imagenet'で学習したモデルを使用（weights='imagenet'）\n",
        "# 今回は10クラスに分類するため、最後尾（トップ）の全結合層は使用しない（include_top=False）\n",
        "# 画像サイズとして最低32×32、カラーチャネルとしてRGBの3の次元が必要なため、\n",
        "# 事前に加工した形状の通りに入力の形状を指定（input_shape=(image_size, image_size, 3)）\n",
        "VGG16TrainedModel = VGG16(weights='imagenet', include_top=False,\n",
        "              input_shape=(32, 32, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UAx5IxTQF34"
      },
      "source": [
        "### VGG16の事前学習済みモデルの概要"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktAk8mj3QG6L"
      },
      "source": [
        "VGG16TrainedModel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCe2v2ilQMTh"
      },
      "source": [
        "### 全結合層の構築"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytMHLhIaQKCK"
      },
      "source": [
        "top_model = Sequential()\n",
        "top_model.add(Flatten(input_shape=VGG16TrainedModel.output_shape[1:]))\n",
        "top_model.add(Dense(256, activation='relu'))\n",
        "top_model.add(Dropout(0.5))\n",
        "top_model.add(Dense(num_class, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47n8Uq08QT1W"
      },
      "source": [
        "### VGG16と全結合層の統合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bp5ePdQQRr5"
      },
      "source": [
        "# 事前学習済みのVGG-16畳み込みニューラルネットワークと、上記作成したレイヤーを結合する\n",
        "Customized_VGG_Model = keras.Model(inputs=VGG16TrainedModel.input, outputs=top_model(VGG16TrainedModel.output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14tPy-fOQZjE"
      },
      "source": [
        "# 事前学習済みのVGG-16畳み込みニューラルネットワークの13番目の層までを学習対象にする\n",
        "for layer in Customized_VGG_Model.layers[:16]:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqRWVd3FVe4-"
      },
      "source": [
        "### 今回の転移学習に使用するモデルの概要"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gZKmUGUVngr"
      },
      "source": [
        "Customized_VGG_Model..summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8AiUrAiQLgT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj7IXpMBQc92"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr4qyMzHQe-_"
      },
      "source": [
        "Customized_VGG_Model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg14RRRAQkf4"
      },
      "source": [
        "Customized_VGG_Model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDqm2qgqQ0jC"
      },
      "source": [
        "Customized_VGG_Model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCBk4JeTYczn"
      },
      "source": [
        "plt.imshow(x_test[99:100].reshape(image_size, image_size, 3), cmap=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7sMfN0eXoGb"
      },
      "source": [
        "class_names[Customized_VGG_Model.predict(x_test[99:100]).argmax()]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}